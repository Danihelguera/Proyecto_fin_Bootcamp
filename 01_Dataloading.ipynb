{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7457ef49",
   "metadata": {},
   "source": [
    "# Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0c5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25feac",
   "metadata": {},
   "source": [
    "# Definimos dataframes donde importamos los csvs y luego consultaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ba44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W_B_df  = pd.DataFrame(columns=['origin', 'destiny', 'dow'  , 'mean_time_s'                                        , 'std_time_s'])\n",
    "#H_B_df  = pd.DataFrame(columns=['origin', 'destiny', 'hod'  , 'mean_time_s_ALL', 'mean_time_s_WD', 'mean_time_s_WE', 'std_time_s_ALL', 'std_time_s_WD', 'std_time_s_WE'])\n",
    "#M_B_df  = pd.DataFrame(columns=['origin', 'destiny', 'Yr_Mt', 'mean_time_s_ALL', 'mean_time_s_WD', 'mean_time_s_WE', 'std_time_s_ALL', 'std_time_s_WD', 'std_time_s_WE'])\n",
    "\n",
    "#W_CP_df = pd.DataFrame(columns=['origin', 'destiny', 'dow'  , 'mean_time_s'                                        , 'std_time_s'])\n",
    "#H_CP_df = pd.DataFrame(columns=['origin', 'destiny', 'hod'  , 'mean_time_s_ALL', 'mean_time_s_WD', 'mean_time_s_WE', 'std_time_s_ALL', 'std_time_s_WD', 'std_time_s_WE'])\n",
    "#M_CP_df = pd.DataFrame(columns=['origin', 'destiny', 'Yr_Mt', 'mean_time_s_ALL', 'mean_time_s_WD', 'mean_time_s_WE', 'std_time_s_ALL', 'std_time_s_WD', 'std_time_s_WE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d233a",
   "metadata": {},
   "source": [
    "# DataFrames WEEKLY (Barrios y Códigos Postales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5151fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['madrid-barrios-2020-1-WeeklyAggregate.csv']\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105472 entries, 0 to 105471\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   origin       105472 non-null  int16\n",
      " 1   destiny      105472 non-null  int16\n",
      " 2   dow          105472 non-null  int16\n",
      " 3   mean_time_s  105472 non-null  int16\n",
      " 4   std_time_s   105472 non-null  int16\n",
      "dtypes: int16(5)\n",
      "memory usage: 1.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Archivos para W_B_df:\n",
    "path_import = \"data/Uber/traveltimes/CSVs_TO_LOAD/W_B/\"\n",
    "path_export = \"data/\"\n",
    "lista_csvs = os.listdir(path_import)\n",
    "print(lista_csvs)\n",
    "print(\"\\n\")\n",
    "\n",
    "W_B_df  = pd.DataFrame().astype('int16')\n",
    "\n",
    "for f in lista_csvs :\n",
    "    data_raw = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt = pd.DataFrame()\n",
    "    data_filt['origin'     ] = data_raw['sourceid'].astype('int16')\n",
    "    data_filt['destiny'    ] = data_raw['dstid'].astype('int16')\n",
    "    data_filt['dow'        ] = data_raw['dow'].astype('int16')\n",
    "    data_filt['mean_time_s'] = data_raw['mean_travel_time'].astype('int16')\n",
    "    data_filt['std_time_s' ] = data_raw['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "    W_B_df = pd.concat([W_B_df, data_filt])\n",
    "    \n",
    "print(W_B_df.info())\n",
    "W_B_df.to_csv(   f\"{path_export}W_B_df.csv\")\n",
    "W_B_df.to_pickle(f\"{path_export}W_B_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9348b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['madrid-codigos_postales-2020-1-WeeklyAggregate.csv']\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74796 entries, 0 to 74795\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   origin       74796 non-null  int16\n",
      " 1   destiny      74796 non-null  int16\n",
      " 2   dow          74796 non-null  int16\n",
      " 3   mean_time_s  74796 non-null  int16\n",
      " 4   std_time_s   74796 non-null  int16\n",
      "dtypes: int16(5)\n",
      "memory usage: 730.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Archivos para W_CP_df:\n",
    "path_import = \"data/Uber/traveltimes/CSVs_TO_LOAD/W_CP/\"\n",
    "path_export = \"data/\"\n",
    "lista_csvs = os.listdir(path_import)\n",
    "print(lista_csvs)\n",
    "print(\"\\n\")\n",
    "\n",
    "W_CP_df  = pd.DataFrame().astype('int16')\n",
    "\n",
    "for f in lista_csvs :\n",
    "    data_raw = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt = pd.DataFrame()\n",
    "    data_filt['origin'     ] = data_raw['sourceid'].astype('int16')\n",
    "    data_filt['destiny'    ] = data_raw['dstid'].astype('int16')\n",
    "    data_filt['dow'        ] = data_raw['dow'].astype('int16')\n",
    "    data_filt['mean_time_s'] = data_raw['mean_travel_time'].astype('int16')\n",
    "    data_filt['std_time_s' ] = data_raw['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "    W_CP_df = pd.concat([W_CP_df, data_filt])\n",
    "    \n",
    "print(W_CP_df.info())\n",
    "W_CP_df.to_csv(   f\"{path_export}W_CP_df.csv\")\n",
    "W_CP_df.to_pickle(f\"{path_export}W_CP_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9444b7e",
   "metadata": {},
   "source": [
    "# DataFrames HOURLY (Barrios y Códigos Postales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c808df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 331379 entries, 0 to 331378\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count   Dtype\n",
      "---  ------           --------------   -----\n",
      " 0   origin           331379 non-null  int16\n",
      " 1   destiny          331379 non-null  int16\n",
      " 2   hod              331379 non-null  int16\n",
      " 3   mean_time_s_ALL  331379 non-null  int16\n",
      " 4   std_time_s_ALL   331379 non-null  int16\n",
      " 5   mean_time_s_WD   331379 non-null  int16\n",
      " 6   std_time_s_WD    331379 non-null  int16\n",
      " 7   mean_time_s_WE   331379 non-null  int16\n",
      " 8   std_time_s_WE    331379 non-null  int16\n",
      "dtypes: int16(9)\n",
      "memory usage: 8.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Archivos para H_B_df:\n",
    "path_import = \"data/Uber/traveltimes/CSVs_TO_LOAD/H_B/\"\n",
    "path_export = \"data/\"\n",
    "lista_csvs = os.listdir(path_import)\n",
    "#print(lista_csvs)\n",
    "\n",
    "f_All = \"madrid-barrios-2020-1-All-HourlyAggregate.csv\"\n",
    "f_WD = \"madrid-barrios-2020-1-OnlyWeekdays-HourlyAggregate.csv\"\n",
    "f_WE = \"madrid-barrios-2020-1-OnlyWeekends-HourlyAggregate.csv\"\n",
    "\n",
    "data_raw_All                     = pd.read_csv(f\"{path_import}{f_All}\")\n",
    "data_filt_All                    = pd.DataFrame()\n",
    "data_filt_All['origin'         ] = data_raw_All['sourceid'].astype('int16')\n",
    "data_filt_All['destiny'        ] = data_raw_All['dstid'].astype('int16')\n",
    "data_filt_All['hod'            ] = data_raw_All['hod'].astype('int16')\n",
    "data_filt_All['mean_time_s_ALL'] = data_raw_All['mean_travel_time'].astype('int16')\n",
    "data_filt_All['std_time_s_ALL' ] = data_raw_All['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "data_raw_WD                      = pd.read_csv(f\"{path_import}{f_WD}\")\n",
    "data_filt_WD                     = pd.DataFrame()\n",
    "data_filt_WD['origin'         ]  = data_raw_WD['sourceid'].astype('int16')\n",
    "data_filt_WD['destiny'        ]  = data_raw_WD['dstid'].astype('int16')\n",
    "data_filt_WD['hod'            ]  = data_raw_WD['hod'].astype('int16')\n",
    "data_filt_WD['mean_time_s_WD']   = data_raw_WD['mean_travel_time'].astype('int16')\n",
    "data_filt_WD['std_time_s_WD' ]   = data_raw_WD['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "data_raw_WE                      = pd.read_csv(f\"{path_import}{f_WE}\")\n",
    "data_filt_WE                     = pd.DataFrame()\n",
    "data_filt_WE['origin'         ]  = data_raw_WE['sourceid'].astype('int16')\n",
    "data_filt_WE['destiny'        ]  = data_raw_WE['dstid'].astype('int16')\n",
    "data_filt_WE['hod'            ]  = data_raw_WE['hod'].astype('int16')\n",
    "data_filt_WE['mean_time_s_WE']   = data_raw_WE['mean_travel_time'].astype('int16')\n",
    "data_filt_WE['std_time_s_WE' ]   = data_raw_WE['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "H_B_df = pd.merge( data_filt_All , data_filt_WD , on=[\"origin\",\"destiny\",\"hod\"], how=\"left\" )\n",
    "H_B_df[\"mean_time_s_WD\"].fillna(H_B_df[\"mean_time_s_ALL\"],inplace=True)\n",
    "H_B_df[\"std_time_s_WD\" ].fillna(H_B_df[\"std_time_s_ALL\" ],inplace=True)\n",
    "H_B_df[\"mean_time_s_WD\"] = H_B_df[\"mean_time_s_WD\"].astype('int16')\n",
    "H_B_df[\"std_time_s_WD\" ] = H_B_df[\"std_time_s_WD\" ].astype('int16')\n",
    "\n",
    "H_B_df = pd.merge( H_B_df , data_filt_WE , on=[\"origin\",\"destiny\",\"hod\"], how=\"left\" )\n",
    "H_B_df[\"mean_time_s_WE\"].fillna(H_B_df[\"mean_time_s_ALL\"],inplace=True)\n",
    "H_B_df[\"std_time_s_WE\" ].fillna(H_B_df[\"std_time_s_ALL\" ],inplace=True)\n",
    "H_B_df[\"mean_time_s_WE\"] = H_B_df[\"mean_time_s_WE\"].astype('int16')\n",
    "H_B_df[\"std_time_s_WE\" ] = H_B_df[\"std_time_s_WE\" ].astype('int16')\n",
    "\n",
    "H_B_df.to_csv(   f\"{path_export}H_B_df.csv\")\n",
    "H_B_df.to_pickle(f\"{path_export}H_B_df.pkl\")\n",
    "H_B_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf2e5a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 208866 entries, 0 to 208865\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count   Dtype\n",
      "---  ------           --------------   -----\n",
      " 0   origin           208866 non-null  int16\n",
      " 1   destiny          208866 non-null  int16\n",
      " 2   hod              208866 non-null  int16\n",
      " 3   mean_time_s_ALL  208866 non-null  int16\n",
      " 4   std_time_s_ALL   208866 non-null  int16\n",
      " 5   mean_time_s_WD   208866 non-null  int16\n",
      " 6   std_time_s_WD    208866 non-null  int16\n",
      " 7   mean_time_s_WE   208866 non-null  int16\n",
      " 8   std_time_s_WE    208866 non-null  int16\n",
      "dtypes: int16(9)\n",
      "memory usage: 5.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Archivos para H_CP_df:\n",
    "path_import = \"data/Uber/traveltimes/CSVs_TO_LOAD/H_CP/\"\n",
    "path_export = \"data/\"\n",
    "lista_csvs = os.listdir(path_import)\n",
    "#print(lista_csvs)\n",
    "\n",
    "f_All = 'madrid-codigos_postales-2020-1-All-HourlyAggregate.csv'\n",
    "f_WD  = 'madrid-codigos_postales-2020-1-OnlyWeekdays-HourlyAggregate.csv'\n",
    "f_WE  = 'madrid-codigos_postales-2020-1-OnlyWeekends-HourlyAggregate.csv' \n",
    "\n",
    "data_raw_All                     = pd.read_csv(f\"{path_import}{f_All}\")\n",
    "data_filt_All                    = pd.DataFrame()\n",
    "data_filt_All['origin'         ] = data_raw_All['sourceid'].astype('int16')\n",
    "data_filt_All['destiny'        ] = data_raw_All['dstid'].astype('int16')\n",
    "data_filt_All['hod'            ] = data_raw_All['hod'].astype('int16')\n",
    "data_filt_All['mean_time_s_ALL'] = data_raw_All['mean_travel_time'].astype('int16')\n",
    "data_filt_All['std_time_s_ALL' ] = data_raw_All['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "data_raw_WD                      = pd.read_csv(f\"{path_import}{f_WD}\")\n",
    "data_filt_WD                     = pd.DataFrame()\n",
    "data_filt_WD['origin'         ]  = data_raw_WD['sourceid'].astype('int16')\n",
    "data_filt_WD['destiny'        ]  = data_raw_WD['dstid'].astype('int16')\n",
    "data_filt_WD['hod'            ]  = data_raw_WD['hod'].astype('int16')\n",
    "data_filt_WD['mean_time_s_WD']   = data_raw_WD['mean_travel_time'].astype('int16')\n",
    "data_filt_WD['std_time_s_WD' ]   = data_raw_WD['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "data_raw_WE                      = pd.read_csv(f\"{path_import}{f_WE}\")\n",
    "data_filt_WE                     = pd.DataFrame()\n",
    "data_filt_WE['origin'         ]  = data_raw_WE['sourceid'].astype('int16')\n",
    "data_filt_WE['destiny'        ]  = data_raw_WE['dstid'].astype('int16')\n",
    "data_filt_WE['hod'            ]  = data_raw_WE['hod'].astype('int16')\n",
    "data_filt_WE['mean_time_s_WE']   = data_raw_WE['mean_travel_time'].astype('int16')\n",
    "data_filt_WE['std_time_s_WE' ]   = data_raw_WE['standard_deviation_travel_time'].astype('int16')\n",
    "\n",
    "H_CP_df = pd.merge( data_filt_All , data_filt_WD , on=[\"origin\",\"destiny\",\"hod\"], how=\"left\" )\n",
    "H_CP_df[\"mean_time_s_WD\"].fillna(H_CP_df[\"mean_time_s_ALL\"],inplace=True)\n",
    "H_CP_df[\"std_time_s_WD\" ].fillna(H_CP_df[\"std_time_s_ALL\" ],inplace=True)\n",
    "H_CP_df[\"mean_time_s_WD\"] = H_CP_df[\"mean_time_s_WD\"].astype('int16')\n",
    "H_CP_df[\"std_time_s_WD\" ] = H_CP_df[\"std_time_s_WD\" ].astype('int16')\n",
    "\n",
    "H_CP_df = pd.merge( H_CP_df , data_filt_WE , on=[\"origin\",\"destiny\",\"hod\"], how=\"left\" )\n",
    "H_CP_df[\"mean_time_s_WE\"].fillna(H_CP_df[\"mean_time_s_ALL\"],inplace=True)\n",
    "H_CP_df[\"std_time_s_WE\" ].fillna(H_CP_df[\"std_time_s_ALL\" ],inplace=True)\n",
    "H_CP_df[\"mean_time_s_WE\"] = H_CP_df[\"mean_time_s_WE\"].astype('int16')\n",
    "H_CP_df[\"std_time_s_WE\" ] = H_CP_df[\"std_time_s_WE\" ].astype('int16')\n",
    "\n",
    "H_CP_df.to_csv(   f\"{path_export}H_CP_df.csv\")\n",
    "H_CP_df.to_pickle(f\"{path_export}H_CP_df.pkl\")\n",
    "H_CP_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa2ada",
   "metadata": {},
   "source": [
    "# DataFrames MONTHLY (Barrios y Códigos Postales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e974004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1- se hacen las listas de ficheros CSV a añadir.\n",
      "\n",
      "2- se leen los ficheros y se hacen los dataframes conjuntos segun All, WD y WE.\n",
      "ALL:\n",
      "WD:\n",
      "WE:\n",
      "\n",
      "3- se juntan (merge) los dataframes de All con WD y con WE.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 534784 entries, 0 to 534783\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   origin           534784 non-null  int16         \n",
      " 1   destiny          534784 non-null  int16         \n",
      " 2   yr-month         534784 non-null  object        \n",
      " 3   date             534784 non-null  datetime64[ns]\n",
      " 4   mean_time_s_ALL  534784 non-null  int16         \n",
      " 5   std_time_s_ALL   534784 non-null  int16         \n",
      " 6   mean_time_s_WD   534784 non-null  int16         \n",
      " 7   std_time_s_WD    534784 non-null  int16         \n",
      " 8   mean_time_s_WE   534784 non-null  int16         \n",
      " 9   std_time_s_WE    534784 non-null  int16         \n",
      "dtypes: datetime64[ns](1), int16(8), object(1)\n",
      "memory usage: 20.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Archivos para M_B_df:\n",
    "print(\"1- se hacen las listas de ficheros CSV a añadir.\\n\")\n",
    "path_import = \"data/Uber/traveltimes/CSVs_TO_LOAD/M_B/\"\n",
    "path_export = \"data/\"\n",
    "lista_csvs = os.listdir(path_import)\n",
    "#print(lista_csvs)\n",
    "lista_csvs_All = []\n",
    "lista_csvs_WD = []\n",
    "lista_csvs_WE = []\n",
    "for f in lista_csvs :\n",
    "    if \"All\" in f :\n",
    "        lista_csvs_All.append(f)\n",
    "    elif \"OnlyWeekdays\" in f :\n",
    "        lista_csvs_WD.append(f)\n",
    "    elif \"OnlyWeekends\" in f :\n",
    "        lista_csvs_WE.append(f)\n",
    "#print(lista_csvs_All)\n",
    "#print(lista_csvs_WD)\n",
    "#print(lista_csvs_WE)\n",
    "\n",
    "print(\"2- se leen los ficheros y se hacen los dataframes conjuntos segun All, WD y WE.\")\n",
    "print(\"ALL:\")\n",
    "data_join_All  = pd.DataFrame().astype('int16')\n",
    "for f in lista_csvs_All :\n",
    "    data_raw_All                     = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt_All                    = pd.DataFrame()\n",
    "    data_filt_All['origin'         ] = data_raw_All['sourceid'].astype('int16')\n",
    "    data_filt_All['destiny'        ] = data_raw_All['dstid'].astype('int16')\n",
    "    data_filt_All['year'           ] = re.findall(r\"\\d{4}\",f)[0]\n",
    "    data_filt_All['month'          ] = data_raw_All['month'].astype('str')\n",
    "    data_filt_All['yr-month'       ] = data_filt_All['year'] + \"-\" + data_filt_All['month']\n",
    "    data_filt_All['day'            ] = 1\n",
    "    data_filt_All['date'           ] = pd.to_datetime(data_filt_All[['year','month','day']])\n",
    "    data_filt_All['mean_time_s_ALL'] = data_raw_All['mean_travel_time'].astype('int16')\n",
    "    data_filt_All['std_time_s_ALL' ] = data_raw_All['standard_deviation_travel_time'].astype('int16')\n",
    "    data_filt_All.drop(columns=[\"year\",\"month\",\"day\"],inplace=True)\n",
    "    data_join_All = pd.concat([data_join_All, data_filt_All])\n",
    "    #print(f\"{f}, --> {len(data_raw_All)} --> total len of joined = {len(data_join_All)}\")\n",
    "#print(data_join_All.info(),\"\\n\")\n",
    "\n",
    "print(\"WD:\")\n",
    "data_join_WD  = pd.DataFrame().astype('int16')\n",
    "for f in lista_csvs_WD :\n",
    "    data_raw_WD                     = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt_WD                    = pd.DataFrame()\n",
    "    data_filt_WD['origin'         ] = data_raw_WD['sourceid'].astype('int16')\n",
    "    data_filt_WD['destiny'        ] = data_raw_WD['dstid'].astype('int16')\n",
    "    data_filt_WD['year'           ] = re.findall(r\"\\d{4}\",f)[0]\n",
    "    data_filt_WD['month'          ] = data_raw_WD['month'].astype('str')\n",
    "    data_filt_WD['yr-month'       ] = data_filt_WD['year'] + \"-\" + data_filt_WD['month']\n",
    "    data_filt_WD['day'            ] = 1\n",
    "    data_filt_WD['date'           ] = pd.to_datetime(data_filt_WD[['year','month','day']])\n",
    "    data_filt_WD['mean_time_s_WD' ] = data_raw_WD['mean_travel_time'].astype('int16')\n",
    "    data_filt_WD['std_time_s_WD'  ] = data_raw_WD['standard_deviation_travel_time'].astype('int16')\n",
    "    data_filt_WD.drop(columns=[\"year\",\"month\",\"day\"],inplace=True)\n",
    "    data_join_WD = pd.concat([data_join_WD, data_filt_WD])\n",
    "    #print(f\"{f}, --> {len(data_raw_WD)} --> total len of joined = {len(data_join_WD)}\")\n",
    "#print(data_join_WD.info(),\"\\n\")\n",
    "\n",
    "print(\"WE:\")\n",
    "data_join_WE  = pd.DataFrame().astype('int16')\n",
    "for f in lista_csvs_WE :\n",
    "    data_raw_WE                     = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt_WE                    = pd.DataFrame()\n",
    "    data_filt_WE['origin'         ] = data_raw_WE['sourceid'].astype('int16')\n",
    "    data_filt_WE['destiny'        ] = data_raw_WE['dstid'].astype('int16')\n",
    "    data_filt_WE['year'           ] = re.findall(r\"\\d{4}\",f)[0]\n",
    "    data_filt_WE['month'          ] = data_raw_WE['month'].astype('str')\n",
    "    data_filt_WE['yr-month'       ] = data_filt_WE['year'] + \"-\" + data_filt_WE['month']\n",
    "    data_filt_WE['day'            ] = 1\n",
    "    data_filt_WE['date'           ] = pd.to_datetime(data_filt_WE[['year','month','day']])\n",
    "    data_filt_WE['mean_time_s_WE' ] = data_raw_WE['mean_travel_time'].astype('int16')\n",
    "    data_filt_WE['std_time_s_WE'  ] = data_raw_WE['standard_deviation_travel_time'].astype('int16')\n",
    "    data_filt_WE.drop(columns=[\"year\",\"month\",\"day\"],inplace=True)\n",
    "    data_join_WE = pd.concat([data_join_WE, data_filt_WE])\n",
    "    #print(f\"{f}, --> {len(data_raw_WE)} --> total len of joined = {len(data_join_WE)}\")\n",
    "#print(data_join_WE.info(),\"\\n\")\n",
    "\n",
    "print(\"\\n3- se juntan (merge) los dataframes de All con WD y con WE.\\n\")\n",
    "\n",
    "df_final = pd.merge( data_join_All , data_join_WD , on=[\"origin\",\"destiny\",\"yr-month\",\"date\"], how=\"left\" )\n",
    "\n",
    "df_final[\"mean_time_s_WD\"].fillna(df_final[\"mean_time_s_ALL\"],inplace=True)\n",
    "df_final[\"std_time_s_WD\" ].fillna(df_final[\"std_time_s_ALL\" ],inplace=True)\n",
    "df_final[\"mean_time_s_WD\"] = df_final[\"mean_time_s_WD\"].astype('int16')\n",
    "df_final[\"std_time_s_WD\" ] = df_final[\"std_time_s_WD\" ].astype('int16')\n",
    "\n",
    "df_final = pd.merge( df_final      , data_join_WE , on=[\"origin\",\"destiny\",\"yr-month\",\"date\"], how=\"left\" )\n",
    "df_final[\"mean_time_s_WE\"].fillna(df_final[\"mean_time_s_ALL\"],inplace=True)\n",
    "df_final[\"std_time_s_WE\" ].fillna(df_final[\"std_time_s_ALL\" ],inplace=True)\n",
    "df_final[\"mean_time_s_WE\"] = df_final[\"mean_time_s_WE\"].astype('int16')\n",
    "df_final[\"std_time_s_WE\" ] = df_final[\"std_time_s_WE\" ].astype('int16')\n",
    "\n",
    "df_final.to_csv(   f\"{path_export}M_B_df.csv\")\n",
    "df_final.to_pickle(f\"{path_export}M_B_df.pkl\")\n",
    "M_B_df = df_final.copy()\n",
    "M_B_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1- se hacen las listas de ficheros CSV a añadir.\n",
      "\n",
      "2- se leen los ficheros y se hacen los dataframes conjuntos segun All, WD y WE.\n",
      "ALL:\n",
      "madrid-codigos_postales-2017-1-All-MonthlyAggregate.csv, --> 10044 --> total len of joined = 10044\n",
      "madrid-codigos_postales-2017-2-All-MonthlyAggregate.csv, --> 13985 --> total len of joined = 24029\n",
      "madrid-codigos_postales-2017-3-All-MonthlyAggregate.csv, --> 15597 --> total len of joined = 39626\n",
      "madrid-codigos_postales-2017-4-All-MonthlyAggregate.csv, --> 18457 --> total len of joined = 58083\n",
      "madrid-codigos_postales-2018-1-All-MonthlyAggregate.csv, --> 19848 --> total len of joined = 77931\n",
      "madrid-codigos_postales-2018-2-All-MonthlyAggregate.csv, --> 22610 --> total len of joined = 100541\n",
      "madrid-codigos_postales-2018-3-All-MonthlyAggregate.csv, --> 26868 --> total len of joined = 127409\n",
      "madrid-codigos_postales-2018-4-All-MonthlyAggregate.csv, --> 29351 --> total len of joined = 156760\n",
      "madrid-codigos_postales-2019-1-All-MonthlyAggregate.csv, --> 30981 --> total len of joined = 187741\n",
      "madrid-codigos_postales-2019-2-All-MonthlyAggregate.csv, --> 34607 --> total len of joined = 222348\n",
      "madrid-codigos_postales-2019-3-All-MonthlyAggregate.csv, --> 34613 --> total len of joined = 256961\n",
      "madrid-codigos_postales-2019-4-All-MonthlyAggregate.csv, --> 35288 --> total len of joined = 292249\n",
      "madrid-codigos_postales-2020-1-All-MonthlyAggregate.csv, --> 32622 --> total len of joined = 324871\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 324871 entries, 0 to 32621\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   origin           324871 non-null  int16         \n",
      " 1   destiny          324871 non-null  int16         \n",
      " 2   yr-month         324871 non-null  object        \n",
      " 3   date             324871 non-null  datetime64[ns]\n",
      " 4   mean_time_s_ALL  324871 non-null  int16         \n",
      " 5   std_time_s_ALL   324871 non-null  int16         \n",
      "dtypes: datetime64[ns](1), int16(4), object(1)\n",
      "memory usage: 9.9+ MB\n",
      "None \n",
      "\n",
      "WD:\n",
      "madrid-codigos_postales-2017-1-OnlyWeekdays-MonthlyAggregate.csv, --> 9918 --> total len of joined = 9918\n",
      "madrid-codigos_postales-2017-2-OnlyWeekdays-MonthlyAggregate.csv, --> 13737 --> total len of joined = 23655\n",
      "madrid-codigos_postales-2017-3-OnlyWeekdays-MonthlyAggregate.csv, --> 15288 --> total len of joined = 38943\n",
      "madrid-codigos_postales-2017-4-OnlyWeekdays-MonthlyAggregate.csv, --> 18071 --> total len of joined = 57014\n",
      "madrid-codigos_postales-2018-1-OnlyWeekdays-MonthlyAggregate.csv, --> 19438 --> total len of joined = 76452\n",
      "madrid-codigos_postales-2018-2-OnlyWeekdays-MonthlyAggregate.csv, --> 22081 --> total len of joined = 98533\n",
      "madrid-codigos_postales-2018-3-OnlyWeekdays-MonthlyAggregate.csv, --> 26387 --> total len of joined = 124920\n",
      "madrid-codigos_postales-2018-4-OnlyWeekdays-MonthlyAggregate.csv, --> 29052 --> total len of joined = 153972\n",
      "madrid-codigos_postales-2019-1-OnlyWeekdays-MonthlyAggregate.csv, --> 30185 --> total len of joined = 184157\n",
      "madrid-codigos_postales-2019-2-OnlyWeekdays-MonthlyAggregate.csv, --> 33638 --> total len of joined = 217795\n",
      "madrid-codigos_postales-2019-3-OnlyWeekdays-MonthlyAggregate.csv, --> 33877 --> total len of joined = 251672\n",
      "madrid-codigos_postales-2019-4-OnlyWeekdays-MonthlyAggregate.csv, --> 34685 --> total len of joined = 286357\n",
      "madrid-codigos_postales-2020-1-OnlyWeekdays-MonthlyAggregate.csv, --> 31962 --> total len of joined = 318319\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 318319 entries, 0 to 31961\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   origin          318319 non-null  int16         \n",
      " 1   destiny         318319 non-null  int16         \n",
      " 2   yr-month        318319 non-null  object        \n",
      " 3   date            318319 non-null  datetime64[ns]\n",
      " 4   mean_time_s_WD  318319 non-null  int16         \n",
      " 5   std_time_s_WD   318319 non-null  int16         \n",
      "dtypes: datetime64[ns](1), int16(4), object(1)\n",
      "memory usage: 9.7+ MB\n",
      "None \n",
      "\n",
      "WE:\n",
      "madrid-codigos_postales-2017-1-OnlyWeekends-MonthlyAggregate.csv, --> 9587 --> total len of joined = 9587\n",
      "madrid-codigos_postales-2017-2-OnlyWeekends-MonthlyAggregate.csv, --> 13527 --> total len of joined = 23114\n",
      "madrid-codigos_postales-2017-3-OnlyWeekends-MonthlyAggregate.csv, --> 15149 --> total len of joined = 38263\n",
      "madrid-codigos_postales-2017-4-OnlyWeekends-MonthlyAggregate.csv, --> 18023 --> total len of joined = 56286\n",
      "madrid-codigos_postales-2018-1-OnlyWeekends-MonthlyAggregate.csv, --> 19336 --> total len of joined = 75622\n",
      "madrid-codigos_postales-2018-2-OnlyWeekends-MonthlyAggregate.csv, --> 22121 --> total len of joined = 97743\n",
      "madrid-codigos_postales-2018-3-OnlyWeekends-MonthlyAggregate.csv, --> 26157 --> total len of joined = 123900\n",
      "madrid-codigos_postales-2018-4-OnlyWeekends-MonthlyAggregate.csv, --> 28653 --> total len of joined = 152553\n",
      "madrid-codigos_postales-2019-1-OnlyWeekends-MonthlyAggregate.csv, --> 30337 --> total len of joined = 182890\n",
      "madrid-codigos_postales-2019-2-OnlyWeekends-MonthlyAggregate.csv, --> 33856 --> total len of joined = 216746\n",
      "madrid-codigos_postales-2019-3-OnlyWeekends-MonthlyAggregate.csv, --> 33827 --> total len of joined = 250573\n",
      "madrid-codigos_postales-2019-4-OnlyWeekends-MonthlyAggregate.csv, --> 34468 --> total len of joined = 285041\n",
      "madrid-codigos_postales-2020-1-OnlyWeekends-MonthlyAggregate.csv, --> 31644 --> total len of joined = 316685\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 316685 entries, 0 to 31643\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   origin          316685 non-null  int16         \n",
      " 1   destiny         316685 non-null  int16         \n",
      " 2   yr-month        316685 non-null  object        \n",
      " 3   date            316685 non-null  datetime64[ns]\n",
      " 4   mean_time_s_WE  316685 non-null  int16         \n",
      " 5   std_time_s_WE   316685 non-null  int16         \n",
      "dtypes: datetime64[ns](1), int16(4), object(1)\n",
      "memory usage: 9.7+ MB\n",
      "None \n",
      "\n",
      "\n",
      "3- se juntan (merge) los dataframes de All con WD y con WE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Archivos para M_CP_df:\n",
    "print(\"1- se hacen las listas de ficheros CSV a añadir.\\n\")\n",
    "path_import = \"data/Uber/traveltimes/CSVs_TO_LOAD/M_CP/\"\n",
    "path_export = \"data/\"\n",
    "lista_csvs = os.listdir(path_import)\n",
    "#print(lista_csvs)\n",
    "lista_csvs_All = []\n",
    "lista_csvs_WD = []\n",
    "lista_csvs_WE = []\n",
    "for f in lista_csvs :\n",
    "    if \"All\" in f :\n",
    "        lista_csvs_All.append(f)\n",
    "    elif \"OnlyWeekdays\" in f :\n",
    "        lista_csvs_WD.append(f)\n",
    "    elif \"OnlyWeekends\" in f :\n",
    "        lista_csvs_WE.append(f)\n",
    "#print(lista_csvs_All)\n",
    "#print(lista_csvs_WD)\n",
    "#print(lista_csvs_WE)\n",
    "\n",
    "print(\"2- se leen los ficheros y se hacen los dataframes conjuntos segun All, WD y WE.\")\n",
    "print(\"ALL:\")\n",
    "data_join_All  = pd.DataFrame().astype('int16')\n",
    "for f in lista_csvs_All :\n",
    "    data_raw_All                     = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt_All                    = pd.DataFrame()\n",
    "    data_filt_All['origin'         ] = data_raw_All['sourceid'].astype('int16')\n",
    "    data_filt_All['destiny'        ] = data_raw_All['dstid'].astype('int16')\n",
    "    data_filt_All['year'           ] = re.findall(r\"\\d{4}\",f)[0]\n",
    "    data_filt_All['month'          ] = data_raw_All['month'].astype('str')\n",
    "    data_filt_All['yr-month'       ] = data_filt_All['year'] + \"-\" + data_filt_All['month']\n",
    "    data_filt_All['day'            ] = 1\n",
    "    data_filt_All['date'           ] = pd.to_datetime(data_filt_All[['year','month','day']])\n",
    "    data_filt_All['mean_time_s_ALL'] = data_raw_All['mean_travel_time'].astype('int16')\n",
    "    data_filt_All['std_time_s_ALL' ] = data_raw_All['standard_deviation_travel_time'].astype('int16')\n",
    "    data_filt_All.drop(columns=[\"year\",\"month\",\"day\"],inplace=True)\n",
    "    data_join_All = pd.concat([data_join_All, data_filt_All])\n",
    "    print(f\"{f}, --> {len(data_raw_All)} --> total len of joined = {len(data_join_All)}\")\n",
    "print(data_join_All.info(),\"\\n\")\n",
    "\n",
    "print(\"WD:\")\n",
    "data_join_WD  = pd.DataFrame().astype('int16')\n",
    "for f in lista_csvs_WD :\n",
    "    data_raw_WD                     = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt_WD                    = pd.DataFrame()\n",
    "    data_filt_WD['origin'         ] = data_raw_WD['sourceid'].astype('int16')\n",
    "    data_filt_WD['destiny'        ] = data_raw_WD['dstid'].astype('int16')\n",
    "    data_filt_WD['year'           ] = re.findall(r\"\\d{4}\",f)[0]\n",
    "    data_filt_WD['month'          ] = data_raw_WD['month'].astype('str')\n",
    "    data_filt_WD['yr-month'       ] = data_filt_WD['year'] + \"-\" + data_filt_WD['month']\n",
    "    data_filt_WD['day'            ] = 1\n",
    "    data_filt_WD['date'           ] = pd.to_datetime(data_filt_WD[['year','month','day']])\n",
    "    data_filt_WD['mean_time_s_WD' ] = data_raw_WD['mean_travel_time'].astype('int16')\n",
    "    data_filt_WD['std_time_s_WD'  ] = data_raw_WD['standard_deviation_travel_time'].astype('int16')\n",
    "    data_filt_WD.drop(columns=[\"year\",\"month\",\"day\"],inplace=True)\n",
    "    data_join_WD = pd.concat([data_join_WD, data_filt_WD])\n",
    "    print(f\"{f}, --> {len(data_raw_WD)} --> total len of joined = {len(data_join_WD)}\")\n",
    "print(data_join_WD.info(),\"\\n\")\n",
    "\n",
    "print(\"WE:\")\n",
    "data_join_WE  = pd.DataFrame().astype('int16')\n",
    "for f in lista_csvs_WE :\n",
    "    data_raw_WE                     = pd.read_csv(f\"{path_import}{f}\")\n",
    "    data_filt_WE                    = pd.DataFrame()\n",
    "    data_filt_WE['origin'         ] = data_raw_WE['sourceid'].astype('int16')\n",
    "    data_filt_WE['destiny'        ] = data_raw_WE['dstid'].astype('int16')\n",
    "    data_filt_WE['year'           ] = re.findall(r\"\\d{4}\",f)[0]\n",
    "    data_filt_WE['month'          ] = data_raw_WE['month'].astype('str')\n",
    "    data_filt_WE['yr-month'       ] = data_filt_WE['year'] + \"-\" + data_filt_WE['month']\n",
    "    data_filt_WE['day'            ] = 1\n",
    "    data_filt_WE['date'           ] = pd.to_datetime(data_filt_WE[['year','month','day']])\n",
    "    data_filt_WE['mean_time_s_WE' ] = data_raw_WE['mean_travel_time'].astype('int16')\n",
    "    data_filt_WE['std_time_s_WE'  ] = data_raw_WE['standard_deviation_travel_time'].astype('int16')\n",
    "    data_filt_WE.drop(columns=[\"year\",\"month\",\"day\"],inplace=True)\n",
    "    data_join_WE = pd.concat([data_join_WE, data_filt_WE])\n",
    "    print(f\"{f}, --> {len(data_raw_WE)} --> total len of joined = {len(data_join_WE)}\")\n",
    "print(data_join_WE.info(),\"\\n\")\n",
    "\n",
    "print(\"\\n3- se juntan (merge) los dataframes de All con WD y con WE.\\n\")\n",
    "\n",
    "df_final = pd.merge( data_join_All , data_join_WD , on=[\"origin\",\"destiny\",\"yr-month\",\"date\"], how=\"left\" )\n",
    "\n",
    "df_final[\"mean_time_s_WD\"].fillna(df_final[\"mean_time_s_ALL\"],inplace=True)\n",
    "df_final[\"std_time_s_WD\" ].fillna(df_final[\"std_time_s_ALL\" ],inplace=True)\n",
    "df_final[\"mean_time_s_WD\"] = df_final[\"mean_time_s_WD\"].astype('int16')\n",
    "df_final[\"std_time_s_WD\" ] = df_final[\"std_time_s_WD\" ].astype('int16')\n",
    "\n",
    "df_final = pd.merge( df_final      , data_join_WE , on=[\"origin\",\"destiny\",\"yr-month\",\"date\"], how=\"left\" )\n",
    "df_final[\"mean_time_s_WE\"].fillna(df_final[\"mean_time_s_ALL\"],inplace=True)\n",
    "df_final[\"std_time_s_WE\" ].fillna(df_final[\"std_time_s_ALL\" ],inplace=True)\n",
    "df_final[\"mean_time_s_WE\"] = df_final[\"mean_time_s_WE\"].astype('int16')\n",
    "df_final[\"std_time_s_WE\" ] = df_final[\"std_time_s_WE\" ].astype('int16')\n",
    "\n",
    "df_final.to_csv(   f\"{path_export}M_CP_df.csv\")\n",
    "df_final.to_pickle(f\"{path_export}M_CP_df.pkl\")\n",
    "M_CP_df = df_final.copy()\n",
    "M_CP_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880fb6a4",
   "metadata": {},
   "source": [
    "# RESUMEN:\n",
    "\n",
    "Se han creado y exportado los archivos tanto en CSV como en PKL.\n",
    "\n",
    "W_CP_df\n",
    "\n",
    "W_B_df\n",
    "\n",
    "M_CP_df\n",
    "\n",
    "M_B_df\n",
    "\n",
    "H_CP_df\n",
    "\n",
    "H_B_df\n",
    "\n",
    "\n",
    "Pasamos a un nuevo Notebook donde los cargaremos y haremos el Análisis Exploratorio de los Datos (EDA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363a396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
